{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0cyvOAheHxG"
      },
      "source": [
        "from PIL import Image\n",
        "img = Image.open('/content/drive/My Drive/Inabia NLP Models/T5.png')\n",
        "\n",
        "resized_im = img.resize((round(img.size[0]*0.9), round(img.size[1]*0.9)))\n",
        "resized_im"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlNF-_DjfgtK"
      },
      "source": [
        "\n",
        "# **Install libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vE1wWifoDu3Y"
      },
      "source": [
        "!pip install pytorch_lightning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjKf77Z7EYpk"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pn-G_eUwFovO"
      },
      "source": [
        "# Check we have a GPU and check the memory size of the GUP\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0n55Ex1Bl2k"
      },
      "source": [
        "# **Import packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5WiQS6FEEsL"
      },
      "source": [
        "import argparse\n",
        "import glob\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import logging\n",
        "import random\n",
        "import re\n",
        "from itertools import chain\n",
        "from string import punctuation\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer,\n",
        "    get_linear_schedule_with_warmup\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ykds8V47B1XT"
      },
      "source": [
        "# **Set a seed**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyrYjMFREUCn"
      },
      "source": [
        "def set_seed(seed):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "\n",
        "set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uSCxnPmCALw"
      },
      "source": [
        "# **T5FineTuner**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zr7mnuYEEhxn"
      },
      "source": [
        "class T5FineTuner(pl.LightningModule):\n",
        "    def __init__(self, hparams):\n",
        "        super(T5FineTuner, self).__init__()\n",
        "        self.hparams = hparams\n",
        "\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained(hparams.model_name_or_path)\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained(hparams.tokenizer_name_or_path)\n",
        "\n",
        "    def is_logger(self):\n",
        "        return True #self.trainer.proc_rank <= 0\n",
        "\n",
        "    def forward(\n",
        "            self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, lm_labels=None\n",
        "    ):\n",
        "        return self.model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            decoder_input_ids=decoder_input_ids,\n",
        "            decoder_attention_mask=decoder_attention_mask,\n",
        "            lm_labels=lm_labels,\n",
        "        )\n",
        "\n",
        "    def _step(self, batch):\n",
        "        lm_labels = batch[\"target_ids\"]\n",
        "        lm_labels[lm_labels[:, :] == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        outputs = self(\n",
        "            input_ids=batch[\"source_ids\"],\n",
        "            attention_mask=batch[\"source_mask\"],\n",
        "            lm_labels=lm_labels,\n",
        "            decoder_attention_mask=batch['target_mask']\n",
        "        )\n",
        "\n",
        "        loss = outputs[0]\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss = self._step(batch)\n",
        "\n",
        "        tensorboard_logs = {\"train_loss\": loss}\n",
        "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
        "\n",
        "    def training_epoch_end(self, outputs):\n",
        "        avg_train_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
        "        tensorboard_logs = {\"avg_train_loss\": avg_train_loss}\n",
        "        return {\"avg_train_loss\": avg_train_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        loss = self._step(batch)\n",
        "        return {\"val_loss\": loss}\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
        "        tensorboard_logs = {\"val_loss\": avg_loss}\n",
        "        return {\"avg_val_loss\": avg_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"Prepare optimizer and schedule (linear warmup and decay)\"\n",
        "\n",
        "        model = self.model\n",
        "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "        optimizer_grouped_parameters = [\n",
        "            {\n",
        "                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": self.hparams.weight_decay,\n",
        "            },\n",
        "            {\n",
        "                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": 0.0,\n",
        "            },\n",
        "        ]\n",
        "        optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n",
        "        self.opt = optimizer\n",
        "        return [optimizer]\n",
        "\n",
        "    def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None, on_tpu=False, using_native_amp=False, using_lbfgs=False):\n",
        "        if self.trainer.use_tpu:\n",
        "            xm.optimizer_step(optimizer)\n",
        "        else:\n",
        "            optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        self.lr_scheduler.step()\n",
        "\n",
        "    def get_tqdm_dict(self):\n",
        "        tqdm_dict = {\"loss\": \"{:.3f}\".format(self.trainer.avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n",
        "\n",
        "        return tqdm_dict\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        train_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"train\", args=self.hparams)\n",
        "        dataloader = DataLoader(train_dataset, batch_size=self.hparams.train_batch_size, drop_last=True, shuffle=True,\n",
        "                                num_workers=4)\n",
        "        t_total = (\n",
        "                (len(dataloader.dataset) // (self.hparams.train_batch_size * max(1, self.hparams.n_gpu)))\n",
        "                // self.hparams.gradient_accumulation_steps\n",
        "                * float(self.hparams.num_train_epochs)\n",
        "        )\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=t_total\n",
        "        )\n",
        "        self.lr_scheduler = scheduler\n",
        "        return dataloader\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        val_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"dev\", args=self.hparams)\n",
        "        return DataLoader(val_dataset, batch_size=self.hparams.eval_batch_size, num_workers=4)\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class LoggingCallback(pl.Callback):\n",
        "  def on_validation_end(self, trainer, pl_module):\n",
        "    logger.info(\"***** Validation results *****\")\n",
        "    if pl_module.is_logger():\n",
        "      metrics = trainer.callback_metrics\n",
        "      # Log results\n",
        "      for key in sorted(metrics):\n",
        "        if key not in [\"log\", \"progress_bar\"]:\n",
        "          logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
        "\n",
        "  def on_test_end(self, trainer, pl_module):\n",
        "    logger.info(\"***** Test results *****\")\n",
        "\n",
        "    if pl_module.is_logger():\n",
        "      metrics = trainer.callback_metrics\n",
        "\n",
        "      # Log and save results to file\n",
        "      output_test_results_file = os.path.join(pl_module.hparams.output_dir, \"test_results.txt\")\n",
        "      with open(output_test_results_file, \"w\") as writer:\n",
        "        for key in sorted(metrics):\n",
        "          if key not in [\"log\", \"progress_bar\"]:\n",
        "            logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
        "            writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nsjLzviCG_A"
      },
      "source": [
        "# **Load datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_3dKvHNFUVL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25637c20-8038-4526-f4a1-1fdc84c4aed8"
      },
      "source": [
        "!mkdir data\n",
        "!wget https://storage.googleapis.com/paws/english/paws_wiki_labeled_final.tar.gz -P data\n",
        "!tar -xvf data/paws_wiki_labeled_final.tar.gz -C data\n",
        "!mv data/final/* data\n",
        "!rm -r data/final\n",
        "!rm -r data/paws_wiki_labeled_final.tar.gz"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-08 06:09:30--  https://storage.googleapis.com/paws/english/paws_wiki_labeled_final.tar.gz\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.204.128, 108.177.12.128, 108.177.13.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.204.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4687157 (4.5M) [application/gzip]\n",
            "Saving to: ‘data/paws_wiki_labeled_final.tar.gz’\n",
            "\n",
            "paws_wiki_labeled_f 100%[===================>]   4.47M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2023-03-08 06:09:30 (53.8 MB/s) - ‘data/paws_wiki_labeled_final.tar.gz’ saved [4687157/4687157]\n",
            "\n",
            "final/test.tsv\n",
            "final/\n",
            "final/train.tsv\n",
            "final/dev.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggbl8IwjHQMi"
      },
      "source": [
        "data_train = pd.read_csv(\"data/train.tsv\", sep=\"\\t\")#.astype(str)\n",
        "data_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mleLBR_XWDf"
      },
      "source": [
        "#data_train_1 = data_train[data_train.label == 1]\n",
        "#data_train_1.sample(100)\n",
        "\n",
        "#data_train_1.to_csv (\"data/For_GPT-3_Test.tsv\", sep='\\t',index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIXOXyLiQmEZ"
      },
      "source": [
        "data_train.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQItP4IRnH7M"
      },
      "source": [
        "data_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4y2tDCjSNjTl"
      },
      "source": [
        "#data_train_1 = data_train[data_train['label']==1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTxYnHZVN9Yt"
      },
      "source": [
        "#data_train_1.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgWCflrKOuBY"
      },
      "source": [
        "data_dev = pd.read_csv(\"data/dev.tsv\", sep=\"\\t\")#.astype(str)\n",
        "#data_dev_1 = data_dev[data_dev['label']==1]\n",
        "\n",
        "data_test = pd.read_csv(\"data/test.tsv\", sep=\"\\t\")#.astype(str)\n",
        "#data_test_1 = data_test[data_test['label']==1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O38qYPeyOJbA"
      },
      "source": [
        "print('Training data: ', data_train.shape)\n",
        "print('Validation data: ', data_dev.shape)\n",
        "print('Test data: ', data_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzJ0RtbmZZiG"
      },
      "source": [
        "#data_train_1 = data_train_1.drop(['id', 'label'], axis=1).reset_index(drop=True)\n",
        "#data_dev_1 = data_dev_1.drop(['id', 'label'], axis=1).reset_index(drop=True)\n",
        "#data_test_1 = data_test_1.drop(['id', 'label'], axis=1).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbcfYt12QMyO"
      },
      "source": [
        "#data_train_1.to_csv (\"data/train.tsv\", sep='\\t',index = False)\n",
        "#data_dev_1.to_csv (\"data/dev.tsv\", sep='\\t',index = False)\n",
        "#data_test_1.to_csv (\"data/test.tsv\", sep='\\t',index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CogSe89eTSs3"
      },
      "source": [
        "#data_dev_1.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4eRy5oVCUny"
      },
      "source": [
        "# **Set arguments**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UZzosIjEr8I"
      },
      "source": [
        "args_dict = dict(\n",
        "    data_dir=\"\", # path for data files\n",
        "    output_dir=\"\", # path to save the checkpoints\n",
        "    model_name_or_path='t5-large',\n",
        "    tokenizer_name_or_path='t5-large',\n",
        "    max_seq_length=256,\n",
        "    learning_rate=3e-4,\n",
        "    weight_decay=0.0,\n",
        "    adam_epsilon=1e-8,\n",
        "    warmup_steps=0,\n",
        "    train_batch_size=1,\n",
        "    eval_batch_size=1,\n",
        "    num_train_epochs=2,\n",
        "    gradient_accumulation_steps=16,\n",
        "    n_gpu=1,\n",
        "    early_stop_callback=False,\n",
        "    fp_16=False, # if you want to enable 16-bit training then install apex and set this to true\n",
        "    opt_level='O1', # you can find out more on optimisation levels here https://nvidia.github.io/apex/amp.html#opt-levels-and-properties\n",
        "    max_grad_norm=1.0, # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n",
        "    seed=42,\n",
        ")\n",
        "\n",
        "train_path = \"data/train.tsv\"\n",
        "val_path = \"data/dev.tsv\"\n",
        "\n",
        "train = pd.read_csv(train_path, sep=\"\\t\").astype(str)\n",
        "print(train.head())\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-large')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_p5OZ7pRCel5"
      },
      "source": [
        "# **ParaphraseDataset()**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5UwYTUAGAo2"
      },
      "source": [
        "class ParaphraseDataset(Dataset):\n",
        "    def __init__(self, tokenizer, data_dir, type_path, max_len=512):\n",
        "        self.path = os.path.join(data_dir, type_path + '.tsv')\n",
        "\n",
        "        self.source_column = \"sentence1\"\n",
        "        self.target_column = \"sentence2\"\n",
        "        self.data = pd.read_csv(self.path, sep=\"\\t\").astype(str)\n",
        "\n",
        "        self.max_len = max_len\n",
        "        self.tokenizer = tokenizer\n",
        "        self.inputs = []\n",
        "        self.targets = []\n",
        "\n",
        "        self._build()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        source_ids = self.inputs[index][\"input_ids\"].squeeze()\n",
        "        target_ids = self.targets[index][\"input_ids\"].squeeze()\n",
        "\n",
        "        src_mask = self.inputs[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
        "        target_mask = self.targets[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
        "\n",
        "        return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask}\n",
        "\n",
        "    def _build(self):\n",
        "        for idx in range(len(self.data)):\n",
        "            input_, target = self.data.loc[idx, self.source_column], self.data.loc[idx, self.target_column]\n",
        "\n",
        "            input_ = \"paraphrase: \"+ input_ + ' </s>'\n",
        "            target = target + \" </s>\"\n",
        "\n",
        "            # tokenize inputs\n",
        "            tokenized_inputs = self.tokenizer.batch_encode_plus(\n",
        "                [input_], max_length=self.max_len, pad_to_max_length=True, return_tensors=\"pt\", truncation='longest_first'\n",
        "            )\n",
        "            # tokenize targets\n",
        "            tokenized_targets = self.tokenizer.batch_encode_plus(\n",
        "                [target], max_length=self.max_len, pad_to_max_length=True, return_tensors=\"pt\", truncation='longest_first'\n",
        "            )\n",
        "\n",
        "            self.inputs.append(tokenized_inputs)\n",
        "            self.targets.append(tokenized_targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkVUkHFMClrH"
      },
      "source": [
        "# **Start training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n41vU8IbGhIM"
      },
      "source": [
        "dataset = ParaphraseDataset(tokenizer, 'data', 'dev', 256)\n",
        "print(\"Val dataset: \",len(dataset))\n",
        "\n",
        "data = dataset[61]\n",
        "print(tokenizer.decode(data['source_ids']))\n",
        "print(tokenizer.decode(data['target_ids']))\n",
        "\n",
        "if not os.path.exists('t5_paraphrase'):\n",
        "    os.makedirs('t5_paraphrase')\n",
        "\n",
        "args_dict.update({'data_dir': 'data', 'output_dir': 't5_paraphrase', 'num_train_epochs':10,'max_seq_length':256})\n",
        "args = argparse.Namespace(**args_dict)\n",
        "print(args_dict)\n",
        "\n",
        "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
        "    filepath=args.output_dir, prefix=\"checkpoint\", monitor=\"val_loss\", mode=\"min\", save_top_k=5\n",
        ")\n",
        "\n",
        "train_params = dict(\n",
        "    accumulate_grad_batches=args.gradient_accumulation_steps,\n",
        "    gpus=args.n_gpu,\n",
        "    max_epochs=args.num_train_epochs,\n",
        " #   early_stop_callback=False,\n",
        "    precision= 16 if args.fp_16 else 32,\n",
        "    amp_level=args.opt_level,\n",
        "    gradient_clip_val=args.max_grad_norm,\n",
        "    checkpoint_callback=checkpoint_callback,\n",
        "    callbacks=[LoggingCallback()],\n",
        ")\n",
        "\n",
        "def get_dataset(tokenizer, type_path, args):\n",
        "  return ParaphraseDataset(tokenizer=tokenizer, data_dir=args.data_dir, type_path=type_path,  max_len=args.max_seq_length)\n",
        "\n",
        "print (\"Initialize model\")\n",
        "model = T5FineTuner(args)\n",
        "\n",
        "trainer = pl.Trainer(**train_params)\n",
        "\n",
        "print (\" Training model\")\n",
        "trainer.fit(model)\n",
        "\n",
        "print (\"training finished\")\n",
        "\n",
        "print (\"Saving model\")\n",
        "model.model.save_pretrained('t5_paraphrase')\n",
        "\n",
        "print (\"Model saved\")\n",
        "\n",
        "!cp \"/content/t5_paraphrase/\" -a \"/content/drive/My Drive/\"\n",
        "!cp \"/content/lightning_logs/\" -a \"/content/drive/My Drive/\"\n",
        "print (\"Copied the final folder to Google Drive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeHmedYU95P5"
      },
      "source": [
        "# **Start testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlvJHBTJ93_n"
      },
      "source": [
        "import torch\n",
        "from transformers import T5ForConditionalGeneration,T5Tokenizer\n",
        "\n",
        "def set_seed(seed):\n",
        "  torch.manual_seed(seed)\n",
        "#  if torch.cuda.is_available():\n",
        "#   torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "best_model_path = \"drive/My Drive/Inabia NLP Models/T5-large-fine-tuned-2 epoch (PAWS)/t5_paraphrase\"\n",
        "model = T5ForConditionalGeneration.from_pretrained(best_model_path)\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-large')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print (\"device \",device)\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyARaz5z_OJM"
      },
      "source": [
        "sentence_1 = \"Washing your hands Properly will keep you away from COVID-19.\"\n",
        "sentence_2 = \"Wikipedia was launched on January 15, 2001, and was created by Jimmy Wales and Larry Sanger.\"\n",
        "sentence_3 = \"NLP is one of the interesting fields for Data Scientists to focus on.\"\n",
        "sentence_4 = \"Do I really need to take a flu shot if I’m healthy with few or no underlying conditions?\"\n",
        "sentence_5 = \"Which course should I take to get started in data science?\"\n",
        "sentence_6 = \"There will be 3 Walmart Black Friday events held in November starting on November 4, November 11 and November 25!\"\n",
        "sentence_7 = \"The FCC says the $200 million civil penalty is the largest fixed-amount penalty in the commission's history.\"\n",
        "sentence_8 = \"Southwest Airlines travelers can now fly directly from San Diego to Honolulu on a new service that took off Wednesday out of the San Diego International Airport.\"\n",
        "sentence_9 = \"Gasoline production averaged 9.1 million bpd last week, slightly down on the previous week.\"\n",
        "sentence_10 = \"If you fall into the latter group, here’s how to replace Google’s new icons for Gmail, Calendar, and other apps with the older, arguably better versions on Android, iPhone, and Chrome.\"\n",
        "sentence_11 = \"Apple has been working on ARM-based Macs for some time, but only made them official at this year's WWDC.\"\n",
        "sentence_12 = \"Microsoft is investigating reports that some users are seeing error 0x80070426 when using their Microsoft account to sign into various apps.\"\n",
        "sentence_13 = \"On Saturday, Connery’s family announced that the Oscar-winning Scottish actor died peacefully in his sleep at home in the Bahamas.\"\n",
        "sentence_14 = \"Baby Shark Dance, from South Korean brand Pinkfong, officially surpassed the song by Luis Fonsi as the most viewed YouTube video of all time, having racked up 7.05 billion views to 7.04 billion.\"\n",
        "sentence_15 = \"The University of Washington has informed the NFL office that due to an increase in COVID-19 infection rate and indications of increased community spread in the local area, NFL personnel are no longer allowed to attend games at Husky Stadium.\"\n",
        "sentence_16 = \"The NBA's basketball-related income was down $1.5 billion last season, according to data provided to teams and obtained by ESPN.\"\n",
        "sentence_17 = \"Yesterday, the huge orbiting laboratory celebrated 20 years of continuous human occupation, a big milestone in humanity's push to extend its footprint into the final frontier.\"\n",
        "sentence_18 = \"A team of researchers led by Osaka University and National Taiwan University created a system of nanoscale silicon resonators that can act as logic gates for light pulses.\"\n",
        "sentence_19 = \"The research on 100 people shows that all had T-cell responses against a range of the coronavirus’s proteins, including the spike protein used as a marker in many vaccine studies, after half a year.\"\n",
        "sentence_20 = \"A group of researchers at MIT recently developed an artificial intelligence model that can detect asymptomatic COVID-19 cases by listening to subtle differences in coughs between healthy people and infected people.\"\n",
        "\n",
        "sentence = sentence_3\n",
        "\n",
        "text =  \"paraphrase: \" + sentence + \" </s>\"\n",
        "\n",
        "max_len = 256\n",
        "\n",
        "encoding = tokenizer.encode_plus(text,pad_to_max_length=True, return_tensors=\"pt\")\n",
        "input_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n",
        "\n",
        "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
        "beam_outputs = model.generate(\n",
        "    input_ids=input_ids, attention_mask=attention_masks,\n",
        "    do_sample=True,\n",
        "    max_length=256,\n",
        "    top_k=120,\n",
        "    top_p=0.98,\n",
        "    early_stopping=True,\n",
        "    num_return_sequences=1\n",
        ")\n",
        "\n",
        "print (\"\\nOriginal sentence: \")\n",
        "print (sentence)\n",
        "print (\"\\n\")\n",
        "print (\"Paraphrased sentences: \")\n",
        "final_outputs =[]\n",
        "for beam_output in beam_outputs:\n",
        "    sent = tokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
        "    if sent.lower() != sentence.lower() and sent not in final_outputs:\n",
        "        final_outputs.append(sent)\n",
        "\n",
        "for i, final_output in enumerate(final_outputs):\n",
        "    print(\"{}: {}\".format(i, final_output))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMFfZPRebQZQ"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkqlI76PIHYQ"
      },
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import T5ForConditionalGeneration,T5Tokenizer, AutoTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZ4ZsE3ZbDiJ"
      },
      "source": [
        "def set_seed(seed):\n",
        "  torch.manual_seed(seed)\n",
        "  if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "best_model_path = \"drive/My Drive/Inabia NLP Models/T5-large-fine-tuned-2 epoch (PAWS)/t5_paraphrase\"\n",
        "model = T5ForConditionalGeneration.from_pretrained(best_model_path)\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-large')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qkkPF5kbvBu"
      },
      "source": [
        "def similarity(x):\n",
        "    input_ids = tokenizer.encode(str(x), return_tensors='pt')\n",
        "    outputs = model.generate(input_ids=input_ids)\n",
        "    similarityd = tokenizer.decode(outputs[0])\n",
        "    return similarityd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSvMnck7b9Eo"
      },
      "source": [
        "que = \"Where is Britain located?\"\n",
        "ans = \"where is England?\"\n",
        "\n",
        "question_str = \"stsb sentence1:\" + que + \" sentence2:\" + ans\n",
        "score = similarity(question_str)\n",
        "#score = pd.to_numeric(score[6:9])\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVbtpEHffEjD"
      },
      "source": [
        "!pip install -q t5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vzj24X_ZfIpm"
      },
      "source": [
        "MODEL = \"small_ssm_nq\" #@param[\"small_ssm_nq\", \"t5.1.1.xl_ssm_nq\", \"t5.1.1.xxl_ssm_nq\"]\n",
        "\n",
        "import os\n",
        "\n",
        "saved_model_dir = f\"drive/My Drive/{MODEL}\"\n",
        "\n",
        "!t5_mesh_transformer \\\n",
        "  --model_dir=\"gs://t5-data/pretrained_models/cbqa/{MODEL}\" \\\n",
        "  --use_model_api \\\n",
        "  --mode=\"export_predict\" \\\n",
        "  --export_dir=\"{saved_model_dir}\"\n",
        "\n",
        "saved_model_path = os.path.join(saved_model_dir, max(os.listdir(saved_model_dir)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IU7HNvvxtNHj"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_text  # Required to run exported model.\n",
        "\n",
        "model = tf.saved_model.load(saved_model_path, [\"serve\"])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}