{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ng-y-bL8vUa3"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
        "api_key=\"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlDxGr4w1hOV"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "file=\"https://s3.amazonaws.com/moonup/production/uploads/1666363435475-62441d1d9fdefb55a0b7d12c.png\"\n",
        "from IPython.core.display import Image, display\n",
        "display(Image(file, width=800, unconfined=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzFKZlqeGErm"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
        "def get_llm_response(question, answer):\n",
        "    template=\"Question: {question}\\n{answer}\"\n",
        "    prompt = PromptTemplate(template=template, input_variables=[\"question\", \"answer\"])\n",
        "    llm_chain = LLMChain(prompt=prompt, llm=HuggingFaceHub(repo_id=\"google/flan-ul2\", model_kwargs={\"temperature\":0.9, \"max_length\":4000}, huggingfacehub_api_token=api_key))\n",
        "    response = llm_chain.run(question=question, answer=answer)\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuFfM423QGZD"
      },
      "source": [
        "# **Text Summarization with chain reasoning**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCuDQ2gqEjtJ"
      },
      "outputs": [],
      "source": [
        "question = \"\"\"MLP policies can struggle to effectively capture the temporal patterns in the non-linear and non-stationary data,\n",
        " leading to the reduced overall performance of the A2C model [52] .\n",
        "  However, the MLP policy can be improved by incorporating a self-attention-based LSTM model, referred to as SA-LSTM policy.\n",
        "   The self-attention mechanism in the LSTM model captures temporal dependencies in financial time series data. \n",
        "   This enables the Actor and Critic networks to understand the hidden representation's temporal importance better and make more informed decisions \n",
        "   based on learned temporal patterns. \n",
        "   Additionally, the self-attention mechanism allows for more sophisticated policy learning, as it can capture complex interactions between the state and action space.\n",
        "    These improvements result in more accurate decision-making by the A2C method.\"\"\"\n",
        "answer_text =\"Rewrite the text using simple english for a 10 year old?\"\n",
        "response = get_llm_response(question, answer_text)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRPScocNQMVa"
      },
      "source": [
        "## **LLM with Augmented data (private PDF and Text files)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ImM5IDoQVMb"
      },
      "outputs": [],
      "source": [
        "!wget  https://raw.githubusercontent.com/Unstructured-IO/unstructured/main/example-docs/layout-parser-paper.pdf -P \".\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ju4T6ElVU52g"
      },
      "source": [
        "Uncomment if these libraries are not installed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IT_EWhyoR_x6"
      },
      "outputs": [],
      "source": [
        "#pip install unstructured\n",
        "#%pip install unstructured[local-inference]\n",
        "#!pip install PyPDF2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-bl71MSzcvP"
      },
      "source": [
        "#Question-Answering from the FLAN5 LLM from Private PDF file (No Open AI here)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8ntQ9CzRQ-q"
      },
      "outputs": [],
      "source": [
        "from PyPDF2 import PdfReader\n",
        "\n",
        "reader = PdfReader(\"/workspace/datasets/langchainexp/layout-parser-paper.pdf\") # Read the uploaded PDF file. Upload your file in collab or read from URL\n",
        "pdf_text = \"\"\n",
        "print(\"Total pages=\",len(reader.pages)) # Total pages in the pdf\n",
        "page_numbers_to_read = [0] # Specify which pages you want to read\n",
        "\n",
        "# Read the given pages\n",
        "for page in page_numbers_to_read:\n",
        "    page_text = reader.pages[page].extract_text()\n",
        "    print(\"Reading pages {} of :\\n{}\".format(page+1, str(len(reader.pages))))\n",
        "    #print(\"The contents of page {} are:\\n{}\".format(page+1, page_text))\n",
        "    pdf_text += page_text\n",
        "    \n",
        "#print(\"All extracted text:\\n\", pdf_text)\n",
        "\n",
        "#save the extracted data from pdf to a txt file\n",
        "file1=open(r\"pdf2text.txt\",\"a\")\n",
        "file1.writelines(pdf_text)\n",
        "print(\"file saved\")\n",
        "\n",
        "\n",
        "# Call the LLM with input data and instruction\n",
        "input_data=pdf_text\n",
        "instruction=\"What are the main findings of this paper?\"\n",
        "response = get_llm_response(input_data, instruction)\n",
        "print(\"LLM Response:\\n\",str(response))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_y3dtJpaydFv"
      },
      "outputs": [],
      "source": [
        "display(response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
